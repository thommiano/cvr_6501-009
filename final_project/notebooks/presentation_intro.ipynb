{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('GTKAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import librosa, librosa.display\n",
    "\n",
    "import time, operator\n",
    "# import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "from deepdream import util as ddutil\n",
    "from deepdream import resnet as ddresnet\n",
    "from deepdream.deepdream import dream\n",
    "\n",
    "from pytorch_utilities.presentation import stream_spectrogram, classify_sound\n",
    "\n",
    "import sys, inspect\n",
    "sys.path.append(\"/audio\")\n",
    "#import torchaudio\n",
    "\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spectrogram(sound_in):\n",
    "    \n",
    "    sound_tensor, sr = torchaudio.load(sound_in)\n",
    "    y = sound_tensor.numpy()[:,0]\n",
    "    #n_mels = 128*3 # perhaps twice as long as 128\n",
    "    #fmax = 10000\n",
    "\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    plt.figure(dpi=100)\n",
    "    librosa.display.specshow(librosa.power_to_db(S,ref=np.max))\n",
    "    \n",
    "    plt.axis('off') # Removes black border\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    del S,y,sr,sound_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Sound-to-Image using transfer learning\n",
    "\n",
    "- By: Thom Miano\n",
    "- Computational Visual Recognition\n",
    "- December 05, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>High-level problem:</b> How can we creatively generate images based on sound inputs from our enviroment, and how do we make this interactive?\n",
    "\n",
    "<b>Machine learning problem:</b> Can you use a Convolutional Neural Network (CNN), like ResNet-50, that's trained on data from a <i>different modality</i> (i.e., images) to fine-tune a reliable enivornmental sound classifier?\n",
    "\n",
    "### Goal: Create images of sounds, train a CNN, creatively generate corresponding images, and give user control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background - Sound examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data-dist](../manuscript/figures/class_histogrograms-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are digital sounds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample rate is the number of samples of audio carried per second, measured in Hz or kHz (one kHz being 1 000 Hz). For example, 44 100 samples per second can be expressed as either 44 100 Hz, or 44.1 kHz. Bandwidth is the difference between the highest and lowest frequencies carried in an audio stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_sound_in = \"/data/datasets/sound_datasets/pytorch_UrbanSound8K/audio/trainset/children_playing/60935-2-0-9.wav\"\n",
    "sound_tensor, sr = torchaudio.load(ex_sound_in)\n",
    "y = sound_tensor.numpy()[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sound_tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Children playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_path = \"/data/datasets/sound_datasets/pytorch_UrbanSound8K/audio/trainset/children_playing/60935-2-0-9.wav\"\n",
    "ipd.Audio(children_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr=44100\n",
    "\n",
    "generate_spectrogram(children_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Street music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_path = \"/data/datasets/sound_datasets/pytorch_UrbanSound8K/audio/trainset/street_music/108041-9-0-5.wav\"\n",
    "ipd.Audio(street_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr=44100\n",
    "\n",
    "generate_spectrogram(street_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drilling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drill_path = \"/data/datasets/sound_datasets/pytorch_UrbanSound8K/audio/trainset/drilling/151005-4-2-1.wav\"\n",
    "ipd.Audio(drill_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr=44100\n",
    "\n",
    "generate_spectrogram(drill_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does the average image look like for the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avg](../manuscript/figures/img_averages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "1. Creating training, validation, and testing dataset using <b>80%, 10%, and 10%</b> and holding class distributions.\n",
    "2. Rescaled spectrograms  from <b>440x600</b> to <b>224x224</b>.\n",
    "3. Several models ranging Epochs up to 100.\n",
    "4. Current model is batch size <b>16, 15 epochs, learning rate of 0.001, momentum of 0.9, weight decay of 0.0005, Stochastic Gradient Descent (SGD) optimizer</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data](../manuscript/figures/example_scaled_spectrograms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cm](../manuscript/figures/testset_cm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../manuscript/figures/training_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![loss](../manuscript/figures/training_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
